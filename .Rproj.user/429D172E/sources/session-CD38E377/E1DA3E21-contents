rm(list = ls())

#install.packages("tidyverse")
#install.packages("kernlab") 
#install.packages("e1071") 
#install.packages("ISLR")  
#install.packages("RColorBrewer")


library(tidyverse)
library(kernlab)
library(e1071)
library(ISLR)
library(RColorBrewer)

# random seed
set.seed(3)

#create our own dataset

x <- matrix(rnorm(20*2),ncol = 2) 
y <-  c(rep(-1,10),rep(1,10))

x[y==1,] <- x[y==1,]+1.5
sampledata1 = data.frame(x=x,y= as.factor(y))

ggplot(data =sampledata1, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")

# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data =sampledata1, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit,sampledata1)

# fit model and produce plot
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'vanilladot')
plot(kernfit,sampledata1= x)

#model 2

# Construct sample data set - not completely separated
set.seed(7)
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
sampledata2 <- data.frame(x=x, y=as.factor(y))

# Plot data set
ggplot(data =sampledata2, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")

# Fit Support Vector Machine model to data set
svmfit2 <- svm(y~., data =sampledata2, kernel = "linear", cost=13)
# Plot Results
plot(svmfit2,sampledata2)

# fit model and produce plot
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'vanilladot', C = 100)
plot(kernfit,sampledata2= x)

# find optimal cost of misclassification
tune.out <- tune(svm, y~., data =sampledata2, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)

# Create a table of misclassified observations
ypred <- predict(bestmod,sampledata2)
#AccuracyTableTranspose <- table(predict = ypred, truth =sampledata2$y)
Accuracytable <- table(Truth = sampledata2$y,predict = ypred)

#accuracy = 
#TPR
#TNR
#FPR
#FNR

# construct larger random data set
set.seed(5)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
sampledata3 <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data =sampledata3, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")

# Fit Support Vector Machine model to data set
svmfit3 <- svm(y~., data =sampledata3, kernel = "radial", cost=10,gamma=1)
# Plot Results
plot(svmfit3,sampledata3)

# fit model and produce plot
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'rbfdot', C = 1, scaled=c())
plot(kernfit,data= x)

#tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = sampledata3, kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000),
                               gamma = c(0.5,1,2,3,4)))
# show best model
tune.out$best.model

# Create a table of misclassified observations
ypred <- predict(bestmod,sampledata3)
#AccuracyTableTranspose <- table(predict = ypred, truth =sampledata2$y)
Accuracytable1 <- table(Truth = sampledata3$y,predict = ypred)

accuracy = (Accuracytable[1,1]+Accuracytable[2,2])/(Accuracytable[1,1]+Accuracytable[1,2]+Accuracytable[2,1]+Accuracytable[2,2])

# construct data set
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y==0,2] <- x[y==0,2] + 2.5
sampledata4 <- data.frame(x=x, y=as.factor(y))
# plot data set
ggplot(data = sampledata4, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme(legend.position = "none")

# fit model
svmfit <- svm(y~., data = sampledata4, kernel = "radial", cost = 10, gamma = 1)
# plot results
plot(svmfit, sampledata4)

# fit model and produce plot
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'rbfdot', C = 1, scaled=c())

# Create a fine grid of the feature space
x.1 <- seq(from = min(sampledata4$x.1), to = max(sampledata4$x.1), length = 100)
x.2 <- seq(from = min(sampledata4$x.2), to = max(sampledata4$x.2), length = 100)
x.grid <- expand.grid(x.2, x.1)

# Get class predictions over grid
pred <- predict(kernfit, newdata = x.grid)

# Plot the results
cols <- brewer.pal(3, "Set1")
plot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))

classes <- matrix(pred, nrow = 100, ncol = 100)
contour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = "", add = TRUE)

points(sampledata4[, 2:1], pch = 19, col = cols[predict(kernfit)])
